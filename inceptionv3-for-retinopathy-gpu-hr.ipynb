{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "c67e806c-e0e6-415b-8cf0-ed92eba5ed37",
    "_uuid": "34b6997bb115a11f47a7f54ce9d9052791b2e707"
   },
   "source": [
    "# Overview\n",
    "The goal is to make a nice retinopathy model by using a pretrained inception v3 as a base and retraining some modified final layers with attention\n",
    "\n",
    "This can be massively improved with \n",
    "* high-resolution images\n",
    "* better data sampling\n",
    "* ensuring there is no leaking between training and validation sets, ```sample(replace = True)``` is real dangerous\n",
    "* better target variable (age) normalization\n",
    "* pretrained models\n",
    "* attention/related techniques to focus on areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e94de3e7-de1f-4c18-ad1f-c8b686127340",
    "_uuid": "c163e45042a69905855f7c04a65676e5aca4837b",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/root/.keras’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# copy the weights and configurations for the pre-trained models\n",
    "!mkdir ~/.keras\n",
    "!mkdir ~/.keras/models\n",
    "!cp ../input/keras-pretrained-models/*notop* ~/.keras/models/\n",
    "!cp ../input/keras-pretrained-models/imagenet_class_index.json ~/.keras/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "c3cc4285-bfa4-4612-ac5f-13d10678c09a",
    "_uuid": "725d378daf5f836d4885d67240fc7955f113309d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # showing and rendering figures\n",
    "# io related\n",
    "from skimage.io import imread\n",
    "import os\n",
    "from glob import glob\n",
    "# not needed in Kaggle, but required in Jupyter\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "c4b38df6-ffa1-4847-b605-511e72b68231",
    "_uuid": "346da81db6ee7a34af8da8af245b42e681f2ba48"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'../input/diabetic-retinopathy-detection/trainLabels.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-82b86c5d7b52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbase_image_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'input'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'diabetic-retinopathy-detection'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mretina_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_image_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trainLabels.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mretina_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PatientId'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m retina_df['path'] = retina_df['image'].map(lambda x: os.path.join(base_image_dir,\n\u001b[1;32m      5\u001b[0m                                                          '{}.jpeg'.format(x)))\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'../input/diabetic-retinopathy-detection/trainLabels.csv' does not exist"
     ]
    }
   ],
   "source": [
    "base_image_dir = os.path.join('..', 'input', 'diabetic-retinopathy-detection')\n",
    "retina_df = pd.read_csv(os.path.join(base_image_dir, 'trainLabels.csv'))\n",
    "retina_df['PatientId'] = retina_df['image'].map(lambda x: x.split('_')[0])\n",
    "retina_df['path'] = retina_df['image'].map(lambda x: os.path.join(base_image_dir,\n",
    "                                                         '{}.jpeg'.format(x)))\n",
    "retina_df['exists'] = retina_df['path'].map(os.path.exists)\n",
    "print(retina_df['exists'].sum(), 'images found of', retina_df.shape[0], 'total')\n",
    "retina_df['eye'] = retina_df['image'].map(lambda x: 1 if x.split('_')[-1]=='left' else 0)\n",
    "from keras.utils.np_utils import to_categorical\n",
    "retina_df['level_cat'] = retina_df['level'].map(lambda x: to_categorical(x, 1+retina_df['level'].max()))\n",
    "\n",
    "retina_df.dropna(inplace = True)\n",
    "retina_df = retina_df[retina_df['exists']]\n",
    "retina_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "818da6ca-bbff-4ca0-ad57-ef3a145ae863",
    "_uuid": "688e4340238e013b8459b6f6470993c7de492d83"
   },
   "source": [
    "# Examine the distribution of eye and severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "5c8bd288-8261-4cbe-a954-e62ac795cc3e",
    "_uuid": "60a8111c4093ca6f69d27a4499442ba7dd750839"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retina_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-02d3a0b4a4c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mretina_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eye'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'retina_df' is not defined"
     ]
    }
   ],
   "source": [
    "retina_df[['level', 'eye']].hist(figsize = (10, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0ba697ed-85bb-4e9a-9765-4c367db078d1",
    "_uuid": "4df45776bae0b8a1bf9d3eb4eaaebce6e24d726d"
   },
   "source": [
    "# Split Data into Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "1192c6b3-a940-4fa0-a498-d7e0d400a796",
    "_uuid": "a48b300ca4d37a6e8b39f82e3c172739635e4baa"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retina_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7e5e7e17d493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrr_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PatientId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'level'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m train_ids, valid_ids = train_test_split(rr_df['PatientId'], \n\u001b[1;32m      4\u001b[0m                                    \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                    \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retina_df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "rr_df = retina_df[['PatientId', 'level']].drop_duplicates()\n",
    "train_ids, valid_ids = train_test_split(rr_df['PatientId'], \n",
    "                                   test_size = 0.25, \n",
    "                                   random_state = 2018,\n",
    "                                   stratify = rr_df['level'])\n",
    "raw_train_df = retina_df[retina_df['PatientId'].isin(train_ids)]\n",
    "valid_df = retina_df[retina_df['PatientId'].isin(valid_ids)]\n",
    "print('train', raw_train_df.shape[0], 'validation', valid_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8060459-da1e-4293-8f61-c7f99de1de9f",
    "_uuid": "26e566d6cec5bd41f9afe392f456ddf7ceb306ea"
   },
   "source": [
    "# Balance the distribution in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "7a130199-fbf6-4c60-95f5-0797b2f3eaf1",
    "_uuid": "ba7befa238b8c11f9672e3539ac58f3da6955bd9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8614a0707445>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m train_df = raw_train_df.groupby(['level', 'eye']).apply(lambda x: x.sample(75, replace = True)\n\u001b[0m\u001b[1;32m      2\u001b[0m                                                       ).reset_index(drop = True)\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'New Data Size:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Old Size:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'eye'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_train_df' is not defined"
     ]
    }
   ],
   "source": [
    "train_df = raw_train_df.groupby(['level', 'eye']).apply(lambda x: x.sample(75, replace = True)\n",
    "                                                      ).reset_index(drop = True)\n",
    "print('New Data Size:', train_df.shape[0], 'Old Size:', raw_train_df.shape[0])\n",
    "train_df[['level', 'eye']].hist(figsize = (10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "9954bfda-29bd-4c4d-b526-0a972b3e43e2",
    "_uuid": "9529ab766763a9f122786464c24ab1ebe22c6006"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "import numpy as np\n",
    "IMG_SIZE = (512, 512) # slightly smaller than vgg16 normally expects\n",
    "def tf_image_loader(out_size, \n",
    "                      horizontal_flip = True, \n",
    "                      vertical_flip = False, \n",
    "                     random_brightness = True,\n",
    "                     random_contrast = True,\n",
    "                    random_saturation = True,\n",
    "                    random_hue = True,\n",
    "                      color_mode = 'rgb',\n",
    "                       preproc_func = preprocess_input,\n",
    "                       on_batch = False):\n",
    "    def _func(X):\n",
    "        with tf.name_scope('image_augmentation'):\n",
    "            with tf.name_scope('input'):\n",
    "                X = tf.image.decode_png(tf.read_file(X), channels = 3 if color_mode == 'rgb' else 0)\n",
    "                X = tf.image.resize_images(X, out_size)\n",
    "            with tf.name_scope('augmentation'):\n",
    "                if horizontal_flip:\n",
    "                    X = tf.image.random_flip_left_right(X)\n",
    "                if vertical_flip:\n",
    "                    X = tf.image.random_flip_up_down(X)\n",
    "                if random_brightness:\n",
    "                    X = tf.image.random_brightness(X, max_delta = 0.1)\n",
    "                if random_saturation:\n",
    "                    X = tf.image.random_saturation(X, lower = 0.75, upper = 1.5)\n",
    "                if random_hue:\n",
    "                    X = tf.image.random_hue(X, max_delta = 0.15)\n",
    "                if random_contrast:\n",
    "                    X = tf.image.random_contrast(X, lower = 0.75, upper = 1.5)\n",
    "                return preproc_func(X)\n",
    "    if on_batch: \n",
    "        # we are meant to use it on a batch\n",
    "        def _batch_func(X, y):\n",
    "            return tf.map_fn(_func, X), y\n",
    "        return _batch_func\n",
    "    else:\n",
    "        # we apply it to everything\n",
    "        def _all_func(X, y):\n",
    "            return _func(X), y         \n",
    "        return _all_func\n",
    "    \n",
    "def tf_augmentor(out_size,\n",
    "                intermediate_size = (640, 640),\n",
    "                 intermediate_trans = 'crop',\n",
    "                 batch_size = 16,\n",
    "                   horizontal_flip = True, \n",
    "                  vertical_flip = False, \n",
    "                 random_brightness = True,\n",
    "                 random_contrast = True,\n",
    "                 random_saturation = True,\n",
    "                    random_hue = True,\n",
    "                  color_mode = 'rgb',\n",
    "                   preproc_func = preprocess_input,\n",
    "                   min_crop_percent = 0.001,\n",
    "                   max_crop_percent = 0.005,\n",
    "                   crop_probability = 0.5,\n",
    "                   rotation_range = 10):\n",
    "    \n",
    "    load_ops = tf_image_loader(out_size = intermediate_size, \n",
    "                               horizontal_flip=horizontal_flip, \n",
    "                               vertical_flip=vertical_flip, \n",
    "                               random_brightness = random_brightness,\n",
    "                               random_contrast = random_contrast,\n",
    "                               random_saturation = random_saturation,\n",
    "                               random_hue = random_hue,\n",
    "                               color_mode = color_mode,\n",
    "                               preproc_func = preproc_func,\n",
    "                               on_batch=False)\n",
    "    def batch_ops(X, y):\n",
    "        batch_size = tf.shape(X)[0]\n",
    "        with tf.name_scope('transformation'):\n",
    "            # code borrowed from https://becominghuman.ai/data-augmentation-on-gpu-in-tensorflow-13d14ecf2b19\n",
    "            # The list of affine transformations that our image will go under.\n",
    "            # Every element is Nx8 tensor, where N is a batch size.\n",
    "            transforms = []\n",
    "            identity = tf.constant([1, 0, 0, 0, 1, 0, 0, 0], dtype=tf.float32)\n",
    "            if rotation_range > 0:\n",
    "                angle_rad = rotation_range / 180 * np.pi\n",
    "                angles = tf.random_uniform([batch_size], -angle_rad, angle_rad)\n",
    "                transforms += [tf.contrib.image.angles_to_projective_transforms(angles, intermediate_size[0], intermediate_size[1])]\n",
    "\n",
    "            if crop_probability > 0:\n",
    "                crop_pct = tf.random_uniform([batch_size], min_crop_percent, max_crop_percent)\n",
    "                left = tf.random_uniform([batch_size], 0, intermediate_size[0] * (1.0 - crop_pct))\n",
    "                top = tf.random_uniform([batch_size], 0, intermediate_size[1] * (1.0 - crop_pct))\n",
    "                crop_transform = tf.stack([\n",
    "                      crop_pct,\n",
    "                      tf.zeros([batch_size]), top,\n",
    "                      tf.zeros([batch_size]), crop_pct, left,\n",
    "                      tf.zeros([batch_size]),\n",
    "                      tf.zeros([batch_size])\n",
    "                  ], 1)\n",
    "                coin = tf.less(tf.random_uniform([batch_size], 0, 1.0), crop_probability)\n",
    "                transforms += [tf.where(coin, crop_transform, tf.tile(tf.expand_dims(identity, 0), [batch_size, 1]))]\n",
    "            if len(transforms)>0:\n",
    "                X = tf.contrib.image.transform(X,\n",
    "                      tf.contrib.image.compose_transforms(*transforms),\n",
    "                      interpolation='BILINEAR') # or 'NEAREST'\n",
    "            if intermediate_trans=='scale':\n",
    "                X = tf.image.resize_images(X, out_size)\n",
    "            elif intermediate_trans=='crop':\n",
    "                X = tf.image.resize_image_with_crop_or_pad(X, out_size[0], out_size[1])\n",
    "            else:\n",
    "                raise ValueError('Invalid Operation {}'.format(intermediate_trans))\n",
    "            return X, y\n",
    "    def _create_pipeline(in_ds):\n",
    "        batch_ds = in_ds.map(load_ops, num_parallel_calls=4).batch(batch_size)\n",
    "        return batch_ds.map(batch_ops)\n",
    "    return _create_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b5767f42-da63-4737-8f50-749c1a25aa84",
    "_uuid": "07851e798db3d89ba13db7d4b56ab2b759221464",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flow_from_dataframe(idg, \n",
    "                        in_df, \n",
    "                        path_col,\n",
    "                        y_col, \n",
    "                        shuffle = True, \n",
    "                        color_mode = 'rgb'):\n",
    "    files_ds = tf.data.Dataset.from_tensor_slices((in_df[path_col].values, \n",
    "                                                   np.stack(in_df[y_col].values,0)))\n",
    "    in_len = in_df[path_col].values.shape[0]\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            files_ds = files_ds.shuffle(in_len) # shuffle the whole dataset\n",
    "        \n",
    "        next_batch = idg(files_ds).repeat().make_one_shot_iterator().get_next()\n",
    "        for i in range(max(in_len//32,1)):\n",
    "            # NOTE: if we loop here it is 'thread-safe-ish' if we loop on the outside it is completely unsafe\n",
    "            yield K.get_session().run(next_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "810bd229-fec9-43c4-b3bd-afd62e3e9552",
    "_uuid": "1848f5048a9e00668c3778a85deea97f980e4f1c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6e249cb77545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                         batch_size = batch_size)\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m train_gen = flow_from_dataframe(core_idg, train_df, \n\u001b[0m\u001b[1;32m     19\u001b[0m                              \u001b[0mpath_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'path'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                             y_col = 'level_cat')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 48\n",
    "core_idg = tf_augmentor(out_size = IMG_SIZE, \n",
    "                        color_mode = 'rgb', \n",
    "                        vertical_flip = True,\n",
    "                        crop_probability=0.0, # crop doesn't work yet\n",
    "                        batch_size = batch_size) \n",
    "valid_idg = tf_augmentor(out_size = IMG_SIZE, color_mode = 'rgb', \n",
    "                         crop_probability=0.0, \n",
    "                         horizontal_flip = False, \n",
    "                         vertical_flip = False, \n",
    "                         random_brightness = False,\n",
    "                         random_contrast = False,\n",
    "                         random_saturation = False,\n",
    "                         random_hue = False,\n",
    "                         rotation_range = 0,\n",
    "                        batch_size = batch_size)\n",
    "\n",
    "train_gen = flow_from_dataframe(core_idg, train_df, \n",
    "                             path_col = 'path',\n",
    "                            y_col = 'level_cat')\n",
    "\n",
    "valid_gen = flow_from_dataframe(valid_idg, valid_df, \n",
    "                             path_col = 'path',\n",
    "                            y_col = 'level_cat') # we can use much larger batches for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ad184b936d5cebae91a265a247d8e0e25920566"
   },
   "source": [
    "# Validation Set\n",
    "We do not perform augmentation at all on these images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "6810407e25b887dd8b352f1e46fb3faceaa58ab7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-be7890bdb6b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_axs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_ax\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_axs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mc_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_x\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m127\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m127\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mc_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Severity {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_gen' is not defined"
     ]
    }
   ],
   "source": [
    "t_x, t_y = next(valid_gen)\n",
    "fig, m_axs = plt.subplots(2, 4, figsize = (16, 8))\n",
    "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(np.clip(c_x*127+127, 0, 255).astype(np.uint8))\n",
    "    c_ax.set_title('Severity {}'.format(np.argmax(c_y, -1)))\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "34ce892a19c9734511e2da1d0f2552b361dc826d"
   },
   "source": [
    "# Training Set\n",
    "These are augmented and a real mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "2d62234f-aeb0-4eba-8a38-d713d819abf6",
    "_uuid": "8190b4ad60d49fa65af074dd138a19cb8787e983",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_gen' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7861fcf3a890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_axs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_ax\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_axs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mc_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_x\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m127\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m127\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mc_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Severity {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_gen' is not defined"
     ]
    }
   ],
   "source": [
    "t_x, t_y = next(train_gen)\n",
    "fig, m_axs = plt.subplots(2, 4, figsize = (16, 8))\n",
    "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(np.clip(c_x*127+127, 0, 255).astype(np.uint8))\n",
    "    c_ax.set_title('Severity {}'.format(np.argmax(c_y, -1)))\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "da22790a-672c-474e-b118-9eef15b53160",
    "_uuid": "55d665e1e8a8d83b9db005a66a965f8a90c62da1"
   },
   "source": [
    "# Attention Model\n",
    "The basic idea is that a Global Average Pooling is too simplistic since some of the regions are more relevant than others. So we build an attention mechanism to turn pixels in the GAP on an off before the pooling and then rescale (Lambda layer) the results based on the number of pixels. The model could be seen as a sort of 'global weighted average' pooling. There is probably something published about it and it is very similar to the kind of attention models used in NLP.\n",
    "It is largely based on the insight that the winning solution annotated and trained a UNET model to segmenting the hand and transforming it. This seems very tedious if we could just learn attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "eeb36110-0cde-4450-a43c-b8f707adb235",
    "_uuid": "1f0dfaccda346d7bc4758e7329d61028d254a8d6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0778b010f437>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLocallyConnected2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0min_lay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mbase_pretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPTModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mt_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbase_pretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 't_x' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16 as PTModel\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2 as PTModel\n",
    "from keras.applications.inception_v3 import InceptionV3 as PTModel\n",
    "from keras.layers import GlobalAveragePooling2D, Dense, Dropout, Flatten, Input, Conv2D, multiply, LocallyConnected2D, Lambda\n",
    "from keras.models import Model\n",
    "in_lay = Input(t_x.shape[1:])\n",
    "base_pretrained_model = PTModel(input_shape =  t_x.shape[1:], include_top = False, weights = 'imagenet')\n",
    "base_pretrained_model.trainable = False\n",
    "pt_depth = base_pretrained_model.get_output_shape_at(0)[-1]\n",
    "pt_features = base_pretrained_model(in_lay)\n",
    "from keras.layers import BatchNormalization\n",
    "bn_features = BatchNormalization()(pt_features)\n",
    "\n",
    "# here we do an attention mechanism to turn pixels in the GAP on an off\n",
    "\n",
    "attn_layer = Conv2D(64, kernel_size = (1,1), padding = 'same', activation = 'relu')(Dropout(0.5)(bn_features))\n",
    "attn_layer = Conv2D(16, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
    "attn_layer = Conv2D(8, kernel_size = (1,1), padding = 'same', activation = 'relu')(attn_layer)\n",
    "attn_layer = Conv2D(1, \n",
    "                    kernel_size = (1,1), \n",
    "                    padding = 'valid', \n",
    "                    activation = 'sigmoid')(attn_layer)\n",
    "# fan it out to all of the channels\n",
    "up_c2_w = np.ones((1, 1, 1, pt_depth))\n",
    "up_c2 = Conv2D(pt_depth, kernel_size = (1,1), padding = 'same', \n",
    "               activation = 'linear', use_bias = False, weights = [up_c2_w])\n",
    "up_c2.trainable = False\n",
    "attn_layer = up_c2(attn_layer)\n",
    "\n",
    "mask_features = multiply([attn_layer, bn_features])\n",
    "gap_features = GlobalAveragePooling2D()(mask_features)\n",
    "gap_mask = GlobalAveragePooling2D()(attn_layer)\n",
    "# to account for missing values from the attention model\n",
    "gap = Lambda(lambda x: x[0]/x[1], name = 'RescaleGAP')([gap_features, gap_mask])\n",
    "gap_dr = Dropout(0.25)(gap)\n",
    "dr_steps = Dropout(0.25)(Dense(128, activation = 'relu')(gap_dr))\n",
    "out_layer = Dense(t_y.shape[-1], activation = 'softmax')(dr_steps)\n",
    "retina_model = Model(inputs = [in_lay], outputs = [out_layer])\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "def top_2_accuracy(in_gt, in_pred):\n",
    "    return top_k_categorical_accuracy(in_gt, in_pred, k=2)\n",
    "\n",
    "retina_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy',\n",
    "                           metrics = ['categorical_accuracy', top_2_accuracy])\n",
    "retina_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "17803ae1-bed8-41a4-9a2c-e66287a24830",
    "_uuid": "48b9764e16fb5af52aed35c82bae6299e67d5bc7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/Keras-2.1.5-py3.6.egg/keras/callbacks.py:919: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` insted.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "weight_path=\"{}_weights.best.hdf5\".format('retina')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=3, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=6) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "84f7cdec-ca00-460c-9991-55b1f7f02f20",
    "_uuid": "78dfa383c51777377c1f81e42017cbcca5f5736f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf ~/.keras # clean up before starting training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "58a75586-442b-4804-84a6-d63d5a42ea14",
    "_uuid": "b2148479bfe41c5d9fd0faece4c75adea509dabe"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retina_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1010cbf85754>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m retina_model.fit_generator(train_gen, \n\u001b[0m\u001b[1;32m      2\u001b[0m                            \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                            \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                            \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retina_model' is not defined"
     ]
    }
   ],
   "source": [
    "retina_model.fit_generator(train_gen, \n",
    "                           steps_per_epoch = train_df.shape[0]//batch_size,\n",
    "                           validation_data = valid_gen, \n",
    "                           validation_steps = valid_df.shape[0]//batch_size,\n",
    "                              epochs = 25, \n",
    "                              callbacks = callbacks_list,\n",
    "                             workers = 0, # tf-generators are not thread-safe\n",
    "                             use_multiprocessing=False, \n",
    "                             max_queue_size = 0\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "4d0c45b0-bb23-48d2-83eb-bc3990043e26",
    "_uuid": "3a90f05dd206cd76c72d8c6278ebb93da41ee45f",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retina_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f9bb4ac0f710>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# load the best version of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mretina_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mretina_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'full_retina_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retina_model' is not defined"
     ]
    }
   ],
   "source": [
    "# load the best version of the model\n",
    "retina_model.load_weights(weight_path)\n",
    "retina_model.save('full_retina_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "f37dd4d8-ecd6-487a-90d8-74fe14a9a318",
    "_uuid": "2b74f4ab850c6e82549d732b6f0524724b95b53c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'valid_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1807b6346dc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# fresh valid gen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m valid_gen = flow_from_dataframe(valid_idg, valid_df, \n\u001b[0m\u001b[1;32m      5\u001b[0m                              \u001b[0mpath_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'path'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                             y_col = 'level_cat') \n",
      "\u001b[0;31mNameError\u001b[0m: name 'valid_df' is not defined"
     ]
    }
   ],
   "source": [
    "##### create one fixed dataset for evaluating\n",
    "from tqdm import tqdm_notebook\n",
    "# fresh valid gen\n",
    "valid_gen = flow_from_dataframe(valid_idg, valid_df, \n",
    "                             path_col = 'path',\n",
    "                            y_col = 'level_cat') \n",
    "vbatch_count = (valid_df.shape[0]//batch_size-1)\n",
    "out_size = vbatch_count*batch_size\n",
    "test_X = np.zeros((out_size,)+t_x.shape[1:], dtype = np.float32)\n",
    "test_Y = np.zeros((out_size,)+t_y.shape[1:], dtype = np.float32)\n",
    "for i, (c_x, c_y) in zip(tqdm_notebook(range(vbatch_count)), \n",
    "                         valid_gen):\n",
    "    j = i*batch_size\n",
    "    test_X[j:(j+c_x.shape[0])] = c_x\n",
    "    test_Y[j:(j+c_x.shape[0])] = c_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "11f33f0a-61eb-488a-b7ea-4bc9d15ba8f9",
    "_uuid": "cca170eb40bc591f89748ede8aa35de4308faaaf"
   },
   "source": [
    "# Show Attention\n",
    "Did our attention model learn anything useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "e41a063f-35c9-410f-be63-f66b63ff9683",
    "_uuid": "ad5b085d351e79b950bf0c2ddc476799d5b0692f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retina_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-b22b04799870>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# get the attention layer since it is the only one with a single output dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mattn_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mretina_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mc_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_shape_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retina_model' is not defined"
     ]
    }
   ],
   "source": [
    "# get the attention layer since it is the only one with a single output dim\n",
    "for attn_layer in retina_model.layers:\n",
    "    c_shape = attn_layer.get_output_shape_at(0)\n",
    "    if len(c_shape)==4:\n",
    "        if c_shape[-1]==1:\n",
    "            print(attn_layer)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "340eef36-f5b2-4b15-a59f-440061a427eb",
    "_uuid": "00850972ae4298f49ed1838b3fc49c2d8fb07547"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e628b2809294>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrand_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m attn_func = K.function(inputs = [retina_model.get_input_at(0), K.learning_phase()],\n\u001b[1;32m      4\u001b[0m            \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mattn_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_output_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_X' is not defined"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "rand_idx = np.random.choice(range(len(test_X)), size = 6)\n",
    "attn_func = K.function(inputs = [retina_model.get_input_at(0), K.learning_phase()],\n",
    "           outputs = [attn_layer.get_output_at(0)]\n",
    "          )\n",
    "fig, m_axs = plt.subplots(len(rand_idx), 2, figsize = (8, 4*len(rand_idx)))\n",
    "[c_ax.axis('off') for c_ax in m_axs.flatten()]\n",
    "for c_idx, (img_ax, attn_ax) in zip(rand_idx, m_axs):\n",
    "    cur_img = test_X[c_idx:(c_idx+1)]\n",
    "    attn_img = attn_func([cur_img, 0])[0]\n",
    "    img_ax.imshow(np.clip(cur_img[0,:,:,:]*127+127, 0, 255).astype(np.uint8))\n",
    "    attn_ax.imshow(attn_img[0, :, :, 0]/attn_img[0, :, :, 0].max(), cmap = 'viridis', \n",
    "                   vmin = 0, vmax = 1, \n",
    "                   interpolation = 'lanczos')\n",
    "    real_cat = np.argmax(test_Y[c_idx, :])\n",
    "    img_ax.set_title('Eye Image\\nCat:%2d' % (real_cat))\n",
    "    pred_cat = retina_model.predict(cur_img)\n",
    "    attn_ax.set_title('Attention Map\\nPred:%2.2f%%' % (100*pred_cat[0,real_cat]))\n",
    "fig.savefig('attention_map.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "24796de7-b1e9-4b3b-bcc6-d997aa3e6d16",
    "_uuid": "244bac80d1ea2074e47932e367996e32cbab6a3d"
   },
   "source": [
    "# Evaluate the results\n",
    "Here we evaluate the results by loading the best version of the model and seeing how the predictions look on the results. We then visualize spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "d0edaf00-4b7c-4f65-af0b-e5a03b9b8428",
    "_uuid": "b421b6183b1919a7414482f0b1ac611079e45174"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retina_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-22e8b48013f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpred_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretina_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpred_Y_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_Y_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy on Test Data: %2.2f%%'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_Y_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_Y_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retina_model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "pred_Y = retina_model.predict(test_X, batch_size = 32, verbose = True)\n",
    "pred_Y_cat = np.argmax(pred_Y, -1)\n",
    "test_Y_cat = np.argmax(test_Y, -1)\n",
    "print('Accuracy on Test Data: %2.2f%%' % (accuracy_score(test_Y_cat, pred_Y_cat)))\n",
    "print(classification_report(test_Y_cat, pred_Y_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "15189df2-3fed-495e-9661-97bb2b712dfd",
    "_uuid": "10162e055ca7cd52878a289bab377231787ab732"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_Y_cat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e5f5da706651>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m sns.heatmap(confusion_matrix(test_Y_cat, pred_Y_cat), \n\u001b[0m\u001b[1;32m      4\u001b[0m             annot=True, fmt=\"d\", cbar = False, cmap = plt.cm.Blues, vmax = test_X.shape[0]//16)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_Y_cat' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.heatmap(confusion_matrix(test_Y_cat, pred_Y_cat), \n",
    "            annot=True, fmt=\"d\", cbar = False, cmap = plt.cm.Blues, vmax = test_X.shape[0]//16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "70827da6-bf91-4b65-80e9-bf1e6b885db3",
    "_uuid": "12dfe39ea80194062068589699953c6645e285d6"
   },
   "source": [
    "# ROC Curve for healthy vs sick\n",
    "Here we make an ROC curve for healthy (```severity == 0```) and sick (```severity>0```) to see how well the model works at just identifying the disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "829475ab-7db2-4421-b9ad-1a51971fd459",
    "_uuid": "2b2aaee6c83043f721b0c9ed5bc4229eb7165200"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_Y_cat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-f953d2afaabe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msick_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_Y_cat\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msick_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msick_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msick_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_Y_cat' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "sick_vec = test_Y_cat>0\n",
    "sick_score = np.sum(pred_Y[:,1:],1)\n",
    "fpr, tpr, _ = roc_curve(sick_vec, sick_score)\n",
    "fig, ax1 = plt.subplots(1,1, figsize = (6, 6), dpi = 150)\n",
    "ax1.plot(fpr, tpr, 'b.-', label = 'Model Prediction (AUC: %2.2f)' % roc_auc_score(sick_vec, sick_score))\n",
    "ax1.plot(fpr, fpr, 'g-', label = 'Random Guessing')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "c34f049f-b032-45bf-9d5e-a756ecc46a82",
    "_uuid": "ba87d0e7c3a77181487b99ca64d13de2aa8a21ee",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-65df8263e352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm_axs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_ax\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm_axs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mc_ax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m127\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m127\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'bone'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     c_ax.set_title('Actual Severity: {}\\n{}'.format(test_Y_cat[idx], \n\u001b[1;32m      5\u001b[0m                                                            '\\n'.join(['Predicted %02d (%04.1f%%): %s' % (k, 100*v, '*'*int(10*v)) for k, v in sorted(enumerate(pred_Y[idx]), key = lambda x: -1*x[1])])), loc='left')\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_X' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAByYAAARiCAYAAADhgJBEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3U+o5fddxvHnY2IVarWLjCBJtAFT\nahChegmFLqy0QtJFshFJoIhSmo3RhUWIKFXiynZREOKfIBIVbIxdaJBIFlIRxJRMqBaTEBjinwwR\nOtbSTdEY+LrIWC7Tm8zpL+dpzkleLwjc3znf3PnunsWbc+6stQIAAAAAAADQ9G1v9AUAAAAAAACA\nNz9hEgAAAAAAAKgTJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqLtqmJyZP5yZ\nL83MP7/K+zMzvz0zF2bmizPzo/u/JgAcJzsKANvZUQDYzo4CcIh2+cTkQ0lue433b09y8+X/7kny\nu6//WgDwpvFQ7CgAbPVQ7CgAbPVQ7CgAB+aqYXKt9XdJ/us1jtyZ5I/XK55I8s6Z+b59XRAAjpkd\nBYDt7CgAbGdHAThE+/gbk9cneeHU88XLrwEAV2dHAWA7OwoA29lRAL7lrt3D75gzXltnHpy5J698\nLUDe/va3/9h73vOePfzzAByzp5566j/XWufe6Hu8gewoAJvZUTsKwHZ21I4CsN3WHd1HmLyY5MZT\nzzckefGsg2utB5M8mCQnJyfr/Pnze/jnAThmM/Nvb/Qd3mB2FIDN7KgdBWA7O2pHAdhu647u46tc\nH03yM/OK9yX56lrrP/bwewHgrcCOAsB2dhQAtrOjAHzLXfUTkzPzmSQfSHLdzFxM8utJvj1J1lq/\nl+SxJB9OciHJ15L8XOuyAHBs7CgAbGdHAWA7OwrAIbpqmFxr3X2V91eSn9/bjQDgTcSOAsB2dhQA\ntrOjAByifXyVKwAAAAAAAMBrEiYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIA6YRIAAAAAAACoEyYBAAAAAACAOmESAAAAAAAAqBMmAQAAAAAAgDphEgAAAAAAAKgT\nJgEAAAAAAIC6ncLkzNw2M8/NzIWZue+M979/Zj43M1+YmS/OzIf3f1UAOE52FAC2s6MAsI0NBeAQ\nXTVMzsw1SR5IcnuSW5LcPTO3XHHs15I8stZ6b5K7kvzOvi8KAMfIjgLAdnYUALaxoQAcql0+MXlr\nkgtrrefXWi8leTjJnVecWUm++/LP35Pkxf1dEQCOmh0FgO3sKABsY0MBOEi7hMnrk7xw6vni5ddO\n+40kH5mZi0keS/ILZ/2imblnZs7PzPlLly5tuC4AHB07CgDb2VEA2GZvG5rYUQD2Z5cwOWe8tq54\nvjvJQ2utG5J8OMmfzMw3/O611oNrrZO11sm5c+e++dsCwPGxowCwnR0FgG32tqGJHQVgf3YJkxeT\n3Hjq+YZ848f6P5rkkSRZa/1Dku9Mct0+LggAR86OAsB2dhQAtrGhABykXcLkk0lunpmbZuZteeUP\nIT96xZl/T/LBJJmZH8orI+Yz/QBgRwHg9bCjALCNDQXgIF01TK61Xk5yb5LHkzyb5JG11tMzc//M\n3HH52MeTfGxm/inJZ5L87Frryq8GAIC3HDsKANvZUQDYxoYCcKiu3eXQWuuxvPIHkE+/9olTPz+T\n5P37vRoAvDnYUQDYzo4CwDY2FIBDtMtXuQIAAAAAAAC8LsIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQJ0wCAAAAAAAAdcIkAAAAAAAAUCdMAgAAAAAAAHXCJAAA\nAAAAAFAnTAIAAAAAAAB1wiQAAAAAAABQt1OYnJnbZua5mbkwM/e9ypmfnplnZubpmfnT/V4TAI6X\nHQWAbWwoAGxnRwE4RNde7cDMXJPkgSQ/meRikidn5tG11jOnztyc5FeSvH+t9ZWZ+d7WhQHgmNhR\nANjGhgLAdnYUgEO1yycmb01yYa31/FrrpSQPJ7nzijMfS/LAWusrSbLW+tJ+rwkAR8uOAsA2NhQA\ntrOjABykXcLk9UleOPV88fJrp707ybtn5u9n5omZue2sXzQz98zM+Zk5f+nSpW03BoDjYkcBYJu9\nbWhiRwF4y7GjABykXcLknPHauuL52iQ3J/lAkruT/MHMvPMb/qe1Hlxrnay1Ts6dO/fN3hUAjpEd\nBYBt9rahiR0F4C3HjgJwkHYJkxeT3Hjq+YYkL55x5i/XWv+71vqXJM/llVEDgLc6OwoA29hQANjO\njgJwkHYJk08muXlmbpqZtyW5K8mjV5z5iyQ/kSQzc11e+RqA5/d5UQA4UnYUALaxoQCwnR0F4CBd\nNUyutV5Ocm+Sx5M8m+SRtdbTM3P/zNxx+djjSb48M88k+VySX15rfbl1aQA4FnYUALaxoQCwnR0F\n4FDNWld+tfi3xsnJyTp//vwb8m8DcDhm5qm11skbfY9jY0cBSOzoVnYUgMSObmVHAUi27+guX+UK\nAAAAAAAA8LoIkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0w\nCQAAAAAAANQJkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQN1O\nYXJmbpuZ52bmwszc9xrnfmpm1syc7O+KAHDc7CgAbGdHAWAbGwrAIbpqmJyZa5I8kOT2JLckuXtm\nbjnj3DuS/GKSz+/7kgBwrOwoAGxnRwFgGxsKwKHa5ROTtya5sNZ6fq31UpKHk9x5xrnfTPLJJP+9\nx/sBwLGzowCwnR0FgG1sKAAHaZcweX2SF049X7z82tfNzHuT3LjW+qvX+kUzc8/MnJ+Z85cuXfqm\nLwsAR8iOAsB2dhQAttnbhl4+a0cB2ItdwuSc8dr6+psz35bk00k+frVftNZ6cK11stY6OXfu3O63\nBIDjZUcBYDs7CgDb7G1DEzsKwP7sEiYvJrnx1PMNSV489fyOJD+c5G9n5l+TvC/Jo/5YMgAksaMA\n8HrYUQDYxoYCcJB2CZNPJrl5Zm6ambcluSvJo///5lrrq2ut69Za71prvSvJE0nuWGudr9wYAI6L\nHQWA7ewoAGxjQwE4SFcNk2utl5Pcm+TxJM8meWSt9fTM3D8zd7QvCADHzI4CwHZ2FAC2saEAHKpr\ndzm01nosyWNXvPaJVzn7gdd/LQB487CjALCdHQWAbWwoAIdol69yBQAAAAAAAHhdhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBOmAQAAAAAAADqhEkAAAAAAACg\nTpgEAAAAAAAA6oRJAAAAAAAAoE6YBAAAAAAAAOqESQAAAAAAAKBupzA5M7fNzHMzc2Fm7jvj/V+a\nmWdm5osz8zcz8wP7vyoAHCc7CgDb2FAA2M6OAnCIrhomZ+aaJA8kuT3JLUnunplbrjj2hSQna60f\nSfLZJJ/c90UB4BjZUQDYxoYCwHZ2FIBDtcsnJm9NcmGt9fxa66UkDye58/SBtdbn1lpfu/z4RJIb\n9ntNADhadhQAtrGhALCdHQXgIO0SJq9P8sKp54uXX3s1H03y12e9MTP3zMz5mTl/6dKl3W8JAMfL\njgLANnvb0MSOAvCWY0cBOEi7hMk547V15sGZjyQ5SfKps95faz241jpZa52cO3du91sCwPGyowCw\nzd42NLGjALzl2FEADtK1O5y5mOTGU883JHnxykMz86Ekv5rkx9da/7Of6wHA0bOjALCNDQWA7ewo\nAAdpl09MPpnk5pm5aWbeluSuJI+ePjAz703y+0nuWGt9af/XBICjZUcBYBsbCgDb2VEADtJVw+Ra\n6+Uk9yZ5PMmzSR5Zaz09M/fPzB2Xj30qyXcl+fOZ+ceZefRVfh0AvKXYUQDYxoYCwHZ2FIBDtctX\nuWat9ViSx6547ROnfv7Qnu8FAG8adhQAtrGhALCdHQXgEO3yVa4AAAAAAAAAr4swCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABAnTAJAAAAAAAA1AmTAAAAAAAAQJ0wCQAAAAAAANQJ\nkwAAAAAAAECdMAkAAAAAAADUCZMAAAAAAABA3U5hcmZum5nnZubCzNx3xvvfMTN/dvn9z8/Mu/Z9\nUQA4VnYUALazowCwjQ0F4BBdNUzOzDVJHkhye5Jbktw9M7dcceyjSb6y1vrBJJ9O8lv7vigAHCM7\nCgDb2VEA2MaGAnCodvnE5K1JLqy1nl9rvZTk4SR3XnHmziR/dPnnzyb54MzM/q4JAEfLjgLAdnYU\nALaxoQAcpF3C5PX5P/buL8Ty867j+Odr1ijW2kp3hZJEE2FrDb1pHUpF0EpV0lwkN1USKFopDaip\noEWoKFXinSKCEK2rlmjB1uiFLhLJhVYqxZRMKZYmJbDG2gwRurY1N6XW6OPFGct0drJzzrPnO3PO\n7usFC/Pn5+7DQ7Yf4b3nTPLcgc/39r925DNjjBeTvJDkVes4IABsOTsKAPPsKADMsaEAbKQzSzxz\n1L+SGRPPpKoeSPLA/qf/VVWfXuLP5+udTfIfp32ILePO5ri3Oe5tdd9z2gdoZkc3i7+jq3Nnc9zb\n6tzZHDtqR0+Kv6Nz3Nsc97Y6dzbnet7RtW1oYkfXwN/ROe5tde5sjnubM7Wjy4TJvSS3Hfj81iTP\nv8Qze1V1Jskrknzx8G80xriQ5EKSVNXuGGNn5tA3Mve2Onc2x73NcW+rq6rd0z5DMzu6Qdzb6tzZ\nHPe2Onc2x45+3TN2tJE7m+Pe5ri31bmzOdf5jq5tQxM7eq3c2Rz3tjp3Nse9zZnd0WXeyvXJJOer\n6o6qujnJfUkuHnrmYpKf2v/4bUn+foxx5L+uAYAbjB0FgHl2FADm2FAANtKxr5gcY7xYVQ8meTzJ\nTUk+MMZ4qqoeSrI7xriY5I+TfLCqLmXxr2ru6zw0AGwLOwoA8+woAMyxoQBsqmXeyjVjjMeSPHbo\na+878PFXkvz4in/2hRWfZ8G9rc6dzXFvc9zb6q77O7OjG8W9rc6dzXFvq3Nnc677e7OjG8OdzXFv\nc9zb6tzZnOv63po2NLnO762JO5vj3lbnzua4tzlT91ZenQ8AAAAAAAB0W+ZnTAIAAAAAAABck/Yw\nWVV3VdUzVXWpqt57xPe/qar+fP/7H6+q27vPtOmWuLNfrKqnq+pTVfV3VfVdp3HOTXPcvR147m1V\nNapq5yTPt6mWubeq+on9/+aeqqo/O+kzbpol/o5+Z1V9pKo+uf/39O7TOOcmqaoPVNXnq+rTL/H9\nqqrf3b/TT1XVG076jJvKjq7Ojs6xo6uzoXPs6Ors6BwbOseOzrGjq7Ojc+zo6uzoHDs6x47OsaOr\ns6Nz7OjqWnZ0jNH2K4sfrPwvSb47yc1J/jnJnYee+dkk79//+L4kf955pk3/teSd/XCSb9n/+Gdu\n9Dtb9t72n3t5ko8meSLJzmmf+7R/Lfnf2/kkn0zy7fuff8dpn3sL7uxCkp/Z//jOJJ897XOf9q8k\nP5jkDUk+/RLfvzvJ3yapJG9K8vHTPvMm/LKjbXdmRyfubf85O7rCndnQ6Xuzo1femx1d/c5saN+9\n2dGJe9t/zo6ucGd2dPre7OiV92ZHV78zO9p3b3Z04t72n7OjK9yZHZ2+Nzt65b2tfUe7XzH5xiSX\nxhjPjjG+muTDSe499My9Sf5k/+O/TPKWqqrmc22yY+9sjPGRMcaX9z99IsmtJ3zGTbTMf2tJ8htJ\nfjPJV07ycBtsmXt7V5KHxxhfSpIxxudP+IybZpk7G0m+bf/jVyR5/gTPt5HGGB9N8sWrPHJvkj8d\nC08keWVVvfpkTrfR7Ojq7OgcO7o6GzrHjk6wo1Ns6Bw7OseOrs6OzrGjE+zoFDs6x47OsaOrs6Nz\n7OiEjh3tDpO3JHnuwOd7+1878pkxxotJXkjyquZzbbJl7uygd2ZRo290x95bVb0+yW1jjL85yYNt\nuGX+e3tNktdU1ceq6omquuvETreZlrmzX0/y9qraS/JYknefzNG22qr/23ejsKOrs6Nz7OjqbOgc\nO9rDjl7Jhs6xo3Ps6Ors6Bwzc/TPAAAgAElEQVQ72sOOXsmOzrGjc+zo6uzoHDvaY+UdPdN6nMVL\nNw8bE8/cSJa+j6p6e5KdJD/UeqLtcNV7q6pvSPI7Sd5xUgfaEsv893Ymi5f+vzmLf8X1j1X1ujHG\nfzafbVMtc2f3J3lkjPHbVfX9ST64f2f/23+8rWULjmZHV2dH59jR1dnQOXa0hy24kg2dY0fn2NHV\n2dE5drSHPbiSHZ1jR+fY0dXZ0Tl2tMfKe9D9ism9JLcd+PzWXPnS1689U1Vnsnh57NVeFnq9W+bO\nUlU/kuRXktwzxvivEzrbJjvu3l6e5HVJ/qGqPpvFex1f9IOSl/47+tdjjP8eY/xrkmeyGLUb1TJ3\n9s4kjybJGOOfknxzkrMncrrttdT/9t2A7Ojq7OgcO7o6GzrHjvawo1eyoXPs6Bw7ujo7OseO9rCj\nV7Kjc+zoHDu6Ojs6x472WHlHu8Pkk0nOV9UdVXVzFj8I+eKhZy4m+an9j9+W5O/HGDfyv6459s72\nX7r+B1mMl/eGXrjqvY0xXhhjnB1j3D7GuD2L93C/Z4yxezrH3RjL/B39qyx+MHeq6mwWbwPw7Ime\ncrMsc2efS/KWJKmq781iwC6f6Cm3z8UkP1kLb0rywhjj30/7UBvAjq7Ojs6xo6uzoXPsaA87eiUb\nOseOzrGjq7Ojc+xoDzt6JTs6x47OsaOrs6Nz7GiPlXe09a1cxxgvVtWDSR5PclOSD4wxnqqqh5Ls\njjEuJvnjLF4OeymLf1VzX+eZNt2Sd/ZbSb41yV/U4mdKf26Mcc+pHXoDLHlvHLLkvT2e5Meq6ukk\n/5Pkl8YYXzi9U5+uJe/sPUn+sKp+IYuXrb/jRv9/zqvqQ1m8dcTZWrxH+68l+cYkGWO8P4v3bL87\nyaUkX07y06dz0s1iR1dnR+fY0dXZ0Dl2dI4dXZ0NnWNH59jR1dnROXZ0jh1dnR2dY0fn2NHV2dE5\ndnROx47WDX6nAAAAAAAAwAnofitXAAAAAAAAAGESAAAAAAAA6CdMAgAAAAAAAO2ESQAAAAAAAKCd\nMAkAAAAAAAC0EyYBAAAAAACAdsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABo\nJ0wCAAAAAAAA7YRJAAAAAAAAoJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA\n2gmTAAAAAAAAQDthEgAAAAAAAGgnTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAA\ngHbCJAAAAAAAANBOmAQAAAAAAADaCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAA\nAKCdMAkAAAAAAAC0EyYBAAAAAACAdsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAA\nAABoJ0wCAAAAAAAA7YRJAAAAAAAAoJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAA\nAAAA2gmTAAAAAAAAQDthEgAAAAAAAGgnTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAA\nAAAAgHbCJAAAAAAAANBOmAQAAAAAAADaCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAA\nAAAAAKCdMAkAAAAAAAC0EyYBAAAAAACAdsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIA\nAAAAAABoJ0wCAAAAAAAA7YRJAAAAAAAAoJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgE\nAAAAAAAA2gmTAAAAAAAAQDthEgAAAAAAAGgnTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMm\nAQAAAAAAgHbCJAAAAAAAANBOmAQAAAAAAADaCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2E\nSQAAAAAAAKCdMAkAAAAAAAC0EyYBAAAAAACAdsIkAAAAAAAA0O7YMFlVH6iqz1fVp1/i+1VVv1tV\nl6rqU1X1hvUfEwC2kx0FgHl2FADm2VEANtEyr5h8JMldV/n+W5Oc3//1QJLfv/ZjAcB145HYUQCY\n9UjsKADMeiR2FIANc2yYHGN8NMkXr/LIvUn+dCw8keSVVfXqdR0QALaZHQWAeXYUAObZUQA20Tp+\nxuQtSZ478Pne/tcAgOPZUQCYZ0cBYJ4dBeDEnVnD71FHfG0c+WDVA1m8LUBe9rKXfd9rX/vaNfzx\nAGyzT3ziE/8xxjh32uc4RXYUgGl21I4CMM+O2lEA5s3u6DrC5F6S2w58fmuS5496cIxxIcmFJNnZ\n2Rm7u7tr+OMB2GZV9W+nfYZTZkcBmGZH7SgA8+yoHQVg3uyOruOtXC8m+claeFOSF8YY/76G3xcA\nbgR2FADm2VEAmGdHAThxx75isqo+lOTNSc5W1V6SX0vyjUkyxnh/kseS3J3kUpIvJ/nprsMCwLax\nowAwz44CwDw7CsAmOjZMjjHuP+b7I8nPre1EAHAdsaMAMM+OAsA8OwrAJlrHW7kCAAAAAAAAXJUw\nCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmTAAAAAAAAQDthEgAAAAAAAGgn\nTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAAgHbCJAAAAAAAANBOmAQAAAAAAADa\nCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAAAKCdMAkAAAAAAAC0EyYBAAAAAACA\ndsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABoJ0wCAAAAAAAA7YRJAAAAAAAA\noJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmTAAAAAAAAQDthEgAAAAAA\nAGgnTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAAgHbCJAAAAAAAANBOmAQAAAAA\nAADaCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAAAKCdMAkAAAAAAAC0EyYBAAAA\nAACAdsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABoJ0wCAAAAAAAA7YRJAAAA\nAAAAoJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmTAAAAAAAAQDthEgAA\nAAAAAGgnTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAAgHbCJAAAAAAAANBOmAQA\nAAAAAADaCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAAAKCdMAkAAAAAAAC0EyYB\nAAAAAACAdsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABoJ0wCAAAAAAAA7YRJ\nAAAAAAAAoJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmTAAAAAAAAQDth\nEgAAAAAAAGgnTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAAgHbCJAAAAAAAANBO\nmAQAAAAAAADaCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAAAKCdMAkAAAAAAAC0\nWypMVtVdVfVMVV2qqvce8f3vrKqPVNUnq+pTVXX3+o8KANvJjgLAPDsKAHNsKACb6NgwWVU3JXk4\nyVuT3Jnk/qq689Bjv5rk0THG65Pcl+T31n1QANhGdhQA5tlRAJhjQwHYVMu8YvKNSS6NMZ4dY3w1\nyYeT3HvomZHk2/Y/fkWS59d3RADYanYUAObZUQCYY0MB2EjLhMlbkjx34PO9/a8d9OtJ3l5Ve0ke\nS/Luo36jqnqgqnaravfy5csTxwWArWNHAWCeHQWAOWvb0MSOArA+y4TJOuJr49Dn9yd5ZIxxa5K7\nk3ywqq74vccYF8YYO2OMnXPnzq1+WgDYPnYUAObZUQCYs7YNTewoAOuzTJjcS3Lbgc9vzZUv639n\nkkeTZIzxT0m+OcnZdRwQALacHQWAeXYUAObYUAA20jJh8skk56vqjqq6OYsfhHzx0DOfS/KWJKmq\n781ixLymHwDsKABcCzsKAHNsKAAb6dgwOcZ4McmDSR5P8pkkj44xnqqqh6rqnv3H3pPkXVX1z0k+\nlOQdY4zDbw0AADccOwoA8+woAMyxoQBsqjPLPDTGeCyLH4B88GvvO/Dx00l+YL1HA4Drgx0FgHl2\nFADm2FAANtEyb+UKAAAAAAAAcE2ESQAAAAAAAKCdMAkAAAAAAAC0EyYBAAAAAACAdsIkAAAAAAAA\n0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABoJ0wCAAAAAAAA7YRJAAAAAAAAoJ0wCQAAAAAA\nALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmTAAAAAAAAQDthEgAAAAAAAGgnTAIAAAAA\nAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAAgHbCJAAAAAAAANBOmAQAAAAAAADaCZMAAAAA\nAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAAAKCdMAkAAAAAAAC0EyYBAAAAAACAdsIkAAAA\nAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABoJ0wCAAAAAAAA7YRJAAAAAAAAoJ0wCQAA\nAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmTAAAAAAAAQDthEgAAAAAAAGgnTAIA\nAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAAgHbCJAAAAAAAANBOmAQAAAAAAADaCZMA\nAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAAAKCdMAkAAAAAAAC0EyYBAAAAAACAdsIk\nAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABoJ0wCAAAAAAAA7YRJAAAAAAAAoJ0w\nCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmTAAAAAAAAQDthEgAAAAAAAGgn\nTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAAgHbCJAAAAAAAANBOmAQAAAAAAADa\nCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAAAKCdMAkAAAAAAAC0EyYBAAAAAACA\ndsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABoJ0wCAAAAAAAA7YRJAAAAAAAA\noJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmTAAAAAAAAQDthEgAAAAAA\nAGgnTAIAAAAAAADthEkAAAAAAACg3VJhsqruqqpnqupSVb33JZ75iap6uqqeqqo/W+8xAWB72VEA\nmGNDAWCeHQVgE5057oGquinJw0l+NMlekier6uIY4+kDz5xP8stJfmCM8aWq+o6uAwPANrGjADDH\nhgLAPDsKwKZa5hWTb0xyaYzx7Bjjq0k+nOTeQ8+8K8nDY4wvJckY4/PrPSYAbC07CgBzbCgAzLOj\nAGykZcLkLUmeO/D53v7XDnpNktdU1ceq6omquuuo36iqHqiq3aravXz58tyJAWC72FEAmLO2DU3s\nKAA3HDsKwEZaJkzWEV8bhz4/k+R8kjcnuT/JH1XVK6/4PxrjwhhjZ4yxc+7cuVXPCgDbyI4CwJy1\nbWhiRwG44dhRADbSMmFyL8ltBz6/NcnzRzzz12OM/x5j/GuSZ7IYNQC40dlRAJhjQwFgnh0FYCMt\nEyafTHK+qu6oqpuT3Jfk4qFn/irJDydJVZ3N4m0Anl3nQQFgS9lRAJhjQwFgnh0FYCMdGybHGC8m\neTDJ40k+k+TRMcZTVfVQVd2z/9jjSb5QVU8n+UiSXxpjfKHr0ACwLewoAMyxoQAwz44CsKlqjMNv\nLX4ydnZ2xu7u7qn82QBsjqr6xBhj57TPsW3sKACJHZ1lRwFI7OgsOwpAMr+jy7yVKwAAAAAAAMA1\nESYBAAAAAACAdsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABoJ0wCAAAAAAAA\n7YRJAAAAAAAAoJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmTAAAAAAAA\nQDthEgAAAAAAAGgnTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAAgHbCJAAAAAAA\nANBOmAQAAAAAAADaCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAAAKCdMAkAAAAA\nAAC0EyYBAAAAAACAdsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABoJ0wCAAAA\nAAAA7YRJAAAAAAAAoJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmTAAAA\nAAAAQDthEgAAAAAAAGgnTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAAgHbCJAAA\nAAAAANBOmAQAAAAAAADaCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAAAKCdMAkA\nAAAAAAC0EyYBAAAAAACAdsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABoJ0wC\nAAAAAAAA7YRJAAAAAAAAoJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmT\nAAAAAAAAQDthEgAAAAAAAGgnTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAAgHbC\nJAAAAAAAANBOmAQAAAAAAADaCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAAAKCd\nMAkAAAAAAAC0EyYBAAAAAACAdsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABo\nJ0wCAAAAAAAA7YRJAAAAAAAAoJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA\n2gmTAAAAAAAAQDthEgAAAAAAAGgnTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAA\ngHZLhcmququqnqmqS1X13qs897aqGlW1s74jAsB2s6MAMM+OAsAcGwrAJjo2TFbVTUkeTvLWJHcm\nub+q7jziuZcn+fkkH1/3IQFgW9lRAJhnRwFgjg0FYFMt84rJNya5NMZ4dozx1SQfTnLvEc/9RpLf\nTPKVNZ4PALadHQWAeXYUAObYUAA20jJh8pYkzx34fG//a19TVa9PctsY42+u9htV1QNVtVtVu5cv\nX175sACwhewoAMyzowAwZ20buv+sHQVgLZYJk3XE18bXvln1DUl+J8l7jvuNxhgXxhg7Y4ydc+fO\nLX9KANhedhQA5tlRAJiztg1N7CgA67NMmNxLctuBz29N8vyBz1+e5HVJ/qGqPpvkTUku+mHJAJDE\njgLAtbCjADDHhgKwkZYJk08mOV9Vd1TVzUnuS3Lx/785xnhhjHF2jHH7GOP2JE8kuWeMsdtyYgDY\nLnYUAObZUQCYY0MB2EjHhskxxotJHkzyeJLPJHl0jPFUVT1UVfd0HxAAtpkdBYB5dhQA5thQADbV\nmWUeGmM8luSxQ19730s8++ZrPxYAXD/sKADMs6MAMMeGArCJlnkrVwAAAAAAAIBrIkwCAAAAAAAA\n7YRJAAAAAAAAoJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmTAAAAAAAA\nQDthEgAAAAAAAGgnTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAAgHbCJAAAAAAA\nANBOmAQAAAAAAADaCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAAAKCdMAkAAAAA\nAAC0EyYBAAAAAACAdsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABoJ0wCAAAA\nAAAA7YRJAAAAAAAAoJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmTAAAA\nAAAAQDthEgAAAAAAAGgnTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAAgHbCJAAA\nAAAAANBOmAQAAAAAAADaCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAAAKCdMAkA\nAAAAAAC0EyYBAAAAAACAdsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABoJ0wC\nAAAAAAAA7YRJAAAAAAAAoJ0wCQAAAAAAALQTJgEAAAAAAIB2wiQAAAAAAADQTpgEAAAAAAAA2gmT\nAAAAAAAAQDthEgAAAAAAAGgnTAIAAAAAAADthEkAAAAAAACgnTAJAAAAAAAAtBMmAQAAAAAAgHbC\nJAAAAAAAANBOmAQAAAAAAADaCZMAAAAAAABAO2ESAAAAAAAAaCdMAgAAAAAAAO2ESQAAAAAAAKCd\nMAkAAAAAAAC0EyYBAAAAAACAdsIkAAAAAAAA0E6YBAAAAAAAANoJkwAAAAAAAEA7YRIAAAAAAABo\nJ0wCAAAAAAAA7YRJAAAAAAAAoJ0wCQAAAP/X3h2FWHqedQD/P2ZNRVpr6UaQbGwipuJShJYl1Bur\nJEiai92bKBsIthIMVOKFFSFQqBKvbJCCEGhXGqwFTdpc6CIpuWgjFXFDFqIhiQS2sTRLhERbcxOa\nuPp6cQ4ynZnNft+beWe+s/P7wcA5c152Hx7m7H/hf75vAAAAGE4xCQAAAAAAAAynmAQAAAAAAACG\nU0wCAAAAAAAAwykmAQAAAAAAgOEUkwAAAAAAAMBwikkAAAAAAABgOMUkAAAAAAAAMJxiEgAAAAAA\nABhOMQkAAAAAAAAMp5gEAAAAAAAAhlNMAgAAAAAAAMMpJgEAAAAAAIDhFJMAAAAAAADAcIpJAAAA\nAAAAYLhJxWRV3V5VL1bVhaq6f5fXP11VL1TVs1X1jar6wN6PCgCbSY4CQB8ZCgD95CgAS3TFYrKq\nrknyUJKPJzme5K6qOr7t2DNJTrTWfjHJY0k+t9eDAsAmkqMA0EeGAkA/OQrAUk25YvKWJBdaay+1\n1t5K8kiSU1sPtNaebK29sX56LsmxvR0TADaWHAWAPjIUAPrJUQAWaUoxeX2Sl7c8v7j+3uXck+Tr\nu71QVfdW1fmqOv/aa69NnxIANpccBYA+e5ahiRwF4NCRowAs0pRisnb5Xtv1YNXdSU4keXC311tr\nZ1prJ1prJ6677rrpUwLA5pKjANBnzzI0kaMAHDpyFIBFOjLhzMUkN2x5fizJK9sPVdVtST6T5GOt\ntTf3ZjwA2HhyFAD6yFAA6CdHAVikKVdMPp3k5qq6qaquTXI6ydmtB6rqw0m+mORka+3VvR8TADaW\nHAWAPjIUAPrJUQAW6YrFZGvtUpL7kjyR5F+TfLW19nxVPVBVJ9fHHkzy7iRfq6p/rqqzl/njAOBQ\nkaMA0EeGAkA/OQrAUk25lWtaa48neXzb9z675fFtezwXAFw15CgA9JGhANBPjgKwRFNu5QoAAAAA\nAADwjigmAQAAAAAAgOEUkwAAAAAAAMBwikkAAAAAAABgOMUkAAAAAAAAMJxiEgAAAAAAABhOMQkA\nAAAAAAAMp5gEAAAAAAAAhlNMAgAAAAAAAMMpJgEAAAAAAIDhFJMAAAAAAADAcIpJAAAAAAAAYDjF\nJAAAAAAAADCcYhIAAAAAAAAYTjEJAAAAAAAADKeYBAAAAAAAAIZTTAIAAAAAAADDKSYBAAAAAACA\n4RSTAAAAAAAAwHCKSQAAAAAAAGA4xSQAAAAAAAAwnGISAAAAAAAAGE4xCQAAAAAAAAynmAQAAAAA\nAACGU0wCAAAAAAAAwykmAQAAAAAAgOEUkwAAAAAAAMBwikkAAAAAAABgOMUkAAAAAAAAMJxiEgAA\nAAAAABhOMQkAAAAAAAAMp5gEAAAAAAAAhlNMAgAAAAAAAMMpJgEAAAAAAIDhFJMAAAAAAADAcIpJ\nAAAAAAAAYDjFJAAAAAAAADCcYhIAAAAAAAAYTjEJAAAAAAAADKeYBAAAAAAAAIZTTAIAAAAAAADD\nKSYBAAAAAACA4RSTAAAAAAAAwHCKSQAAAAAAAGA4xSQAAAAAAAAwnGISAAAAAAAAGE4xCQAAAAAA\nAAynmAQAAAAAAACGU0wCAAAAAAAAwykmAQAAAAAAgOEUkwAAAAAAAMBwikkAAAAAAABgOMUkAAAA\nAAAAMJxiEgAAAAAAABhOMQkAAAAAAAAMp5gEAAAAAAAAhlNMAgAAAAAAAMMpJgEAAAAAAIDhFJMA\nAAAAAADAcIpJAAAAAAAAYDjFJAAAAAAAADCcYhIAAAAAAAAYTjEJAAAAAAAADKeYBAAAAAAAAIZT\nTAIAAAAAAADDKSYBAAAAAACA4RSTAAAAAAAAwHCKSQAAAAAAAGA4xSQAAAAAAAAwnGISAAAAAAAA\nGE4xCQAAAAAAAAynmAQAAAAAAACGU0wCAAAAAAAAwykmAQAAAAAAgOEUkwAAAAAAAMBwikkAAAAA\nAABgOMUkAAAAAAAAMJxiEgAAAAAAABhOMQkAAAAAAAAMp5gEAAAAAAAAhlNMAgAAAAAAAMMpJgEA\nAAAAAIDhFJMAAAAAAADAcIpJAAAAAAAAYDjFJAAAAAAAADCcYhIAAAAAAAAYTjEJAAAAAAAADKeY\nBAAAAAAAAIZTTAIAAFXtpPwAAAhGSURBVAAAAADDTSomq+r2qnqxqi5U1f27vP6uqnp0/fpTVXXj\nXg8KAJtKjgJAPzkKAH1kKABLdMVisqquSfJQko8nOZ7krqo6vu3YPUm+31r7uSSfT/Inez0oAGwi\nOQoA/eQoAPSRoQAs1ZQrJm9JcqG19lJr7a0kjyQ5te3MqSRfXj9+LMmtVVV7NyYAbCw5CgD95CgA\n9JGhACzSlGLy+iQvb3l+cf29Xc+01i4leT3J+/diQADYcHIUAPrJUQDoI0MBWKQjE87s9imZ1nEm\nVXVvknvXT9+squcm/P38sKNJ/uOgh9gwdtbH3vrY23w/f9ADDCZHl8V7dD4762Nv89lZHzkqR/eL\n92gfe+tjb/PZWZ+rOUf3LEMTOboHvEf72Nt8dtbH3vp05eiUYvJikhu2PD+W5JXLnLlYVUeSvDfJ\n97b/Qa21M0nOJElVnW+tnegZ+jCzt/nsrI+99bG3+arq/EHPMJgcXRB7m8/O+tjbfHbWR47+0Bk5\nOpCd9bG3PvY2n531ucpzdM8yNJGj75Sd9bG3+eysj7316c3RKbdyfTrJzVV1U1Vdm+R0krPbzpxN\n8on14zuTfLO1tuunawDgkJGjANBPjgJAHxkKwCJd8YrJ1tqlqrovyRNJrknycGvt+ap6IMn51trZ\nJF9K8pWqupDVp2pOjxwaADaFHAWAfnIUAPrIUACWasqtXNNaezzJ49u+99ktj3+Q5Ndn/t1nZp5n\nxd7ms7M+9tbH3ua76ncmRxfF3uazsz72Np+d9bnq9yZHF8PO+thbH3ubz876XNV7G5ShyVW+t0Hs\nrI+9zWdnfeytT9feytX5AAAAAAAAwGhTfsckAAAAAAAAwDsyvJisqtur6sWqulBV9+/y+ruq6tH1\n609V1Y2jZ1q6CTv7dFW9UFXPVtU3quoDBzHn0lxpb1vO3VlVrapO7Od8SzVlb1X1G+ufueer6q/2\ne8almfAe/ZmqerKqnlm/T+84iDmXpKoerqpXq+q5y7xeVfVn650+W1Uf2e8Zl0qOzidH+8jR+WRo\nHzk6nxztI0P7yNE+cnQ+OdpHjs4nR/vI0T5ytI8cnU+O9pGj8w3J0dbasK+sfrHyt5P8bJJrk/xL\nkuPbzvxOki+sH59O8ujImZb+NXFnv5rkx9ePP3XYdzZ1b+tz70nyrSTnkpw46LkP+mviz9vNSZ5J\n8r7185866Lk3YGdnknxq/fh4ku8c9NwH/ZXkl5N8JMlzl3n9jiRfT1JJPprkqYOeeQlfcnTYzuRo\nx97W5+TojJ3J0O69ydGde5Oj83cmQ8ftTY527G19To7O2Jkc7d6bHN25Nzk6f2dydNze5GjH3tbn\n5OiMncnR7r3J0Z172/McHX3F5C1JLrTWXmqtvZXkkSSntp05leTL68ePJbm1qmrwXEt2xZ211p5s\nrb2xfnouybF9nnGJpvysJckfJ/lckh/s53ALNmVvv53kodba95OktfbqPs+4NFN21pL8xPrxe5O8\nso/zLVJr7VtJvvc2R04l+cu2ci7JT1bVT+/PdIsmR+eTo33k6HwytI8c7SBHu8jQPnK0jxydT472\nkaMd5GgXOdpHjvaRo/PJ0T5ytMOIHB1dTF6f5OUtzy+uv7frmdbapSSvJ3n/4LmWbMrOtronqzb6\nsLvi3qrqw0luaK393X4OtnBTft4+mOSDVfWPVXWuqm7ft+mWacrO/ijJ3VV1McnjSX53f0bbaHP/\n7Tss5Oh8crSPHJ1PhvaRo2PI0Z1kaB852keOzidH+8jRMeToTnK0jxztI0fnk6N95OgYs3P0yNBx\nVpdubtc6zhwmk/dRVXcnOZHkY0Mn2gxvu7eq+pEkn0/yyf0aaENM+Xk7ktWl/7+S1ae4/qGqPtRa\n+6/Bsy3VlJ3dleQvWmt/WlW/lOQr65397/jxNpYs2J0cnU+O9pGj88nQPnJ0DFmwkwztI0f7yNH5\n5GgfOTqGPNhJjvaRo33k6HxytI8cHWN2Hoy+YvJikhu2PD+WnZe+/v+ZqjqS1eWxb3dZ6NVuys5S\nVbcl+UySk621N/dptiW70t7ek+RDSf6+qr6T1b2Oz/pFyZPfo3/bWvvv1tq/JXkxq1A7rKbs7J4k\nX02S1to/JfmxJEf3ZbrNNenfvkNIjs4nR/vI0flkaB85OoYc3UmG9pGjfeTofHK0jxwdQ47uJEf7\nyNE+cnQ+OdpHjo4xO0dHF5NPJ7m5qm6qqmuz+kXIZ7edOZvkE+vHdyb5ZmvtMH+65oo7W1+6/sWs\nwsu9oVfedm+ttddba0dbaze21m7M6h7uJ1tr5w9m3MWY8h79m6x+MXeq6mhWtwF4aV+nXJYpO/tu\nkluTpKp+IasAe21fp9w8Z5P8Zq18NMnrrbV/P+ihFkCOzidH+8jR+WRoHzk6hhzdSYb2kaN95Oh8\ncrSPHB1Dju4kR/vI0T5ydD452keOjjE7R4feyrW1dqmq7kvyRJJrkjzcWnu+qh5Icr61djbJl7K6\nHPZCVp+qOT1ypqWbuLMHk7w7yddq9Tulv9taO3lgQy/AxL2xzcS9PZHk16rqhST/k+QPWmv/eXBT\nH6yJO/v9JH9eVb+X1WXrnzzs/zmvqr/O6tYRR2t1j/Y/TPKjSdJa+0JW92y/I8mFJG8k+a2DmXRZ\n5Oh8crSPHJ1PhvaRo33k6HwytI8c7SNH55OjfeRoHzk6nxztI0f7yNH55GgfOdpnRI7WId8pAAAA\nAAAAsA9G38oVAAAAAAAAQDEJAAAAAAAAjKeYBAAAAAAAAIZTTAIAAAAAAADDKSYBAAAAAACA4RST\nAAAAAAAAwHCKSQAAAAAAAGA4xSQAAAAAAAAw3P8BsiWZ68iRLfMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1473f589b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, m_axs = plt.subplots(2, 4, figsize = (32, 20))\n",
    "for (idx, c_ax) in enumerate(m_axs.flatten()):\n",
    "    c_ax.imshow(np.clip(test_X[idx]*127+127,0 , 255).astype(np.uint8), cmap = 'bone')\n",
    "    c_ax.set_title('Actual Severity: {}\\n{}'.format(test_Y_cat[idx], \n",
    "                                                           '\\n'.join(['Predicted %02d (%04.1f%%): %s' % (k, 100*v, '*'*int(10*v)) for k, v in sorted(enumerate(pred_Y[idx]), key = lambda x: -1*x[1])])), loc='left')\n",
    "    c_ax.axis('off')\n",
    "fig.savefig('trained_img_predictions.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "f2e189dc-f80a-4b16-bb1d-5c05a155a80b",
    "_uuid": "eb6752295030ba512263433f8383711e4ca1c14c",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
